---
title: "Ejercicios capitulos 4 y 5"
author: "Sofia Terra, Santiago Robatto"
date: "today"
format: pdf
mainfont: "Calibri"
pdf-engine: xelatex
---

```{r, echo= FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(bayesrules)
library(tidyverse)
library(janitor)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
#Carga del dataset
data(bechdel, package = "bayesrules")
```

## Ejercicio 4.19

Lo primero que se realizó fue cargar los datos y filtrarlos por el año de interés (1980).

```{r, echo= FALSE, warning=FALSE, message=FALSE}
#Filtramos 1980 
bechdel %>% 
  filter(year == 1980) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

Generamos el gráfico de prior, la verosimilitud y el posterior con plot_beta_binomial:

```{r, echo=FALSE, warning=FALSE, message=FALSE}

plot_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)


```

Se observa que la verosimilitud es igual al posterior. Esto ocurre porque el prior es una beta (1,1) que es uniforme en el recorrido. Interpretando esto de una manera bayesiana, signfiica que no aporta nada de información. Se utiliza para repersentar neutralidad absoltua. Siempre que partimos de una beta con tales parámetros, la verosimilud será igual al posterior.

El posterior alcanza su moda en torno a 0.25 y deja prácticamente con probabilidad 0 a los valores superiores a 0.5. Esto nos da indicios de que la varianza será considerablemente baja.

Para el **Cálculo teórico de la esperanza y modo** utilizaremos el cálculo del posterior del modelo beta binomial, que sabemos que distribuye Beta ($\alpha$+y,$\beta$+n-y).

Dada dicha distribución, sabemos que la esperanza del posterior será:

$$
\mathbb{E}[\pi] = \frac{\alpha + y}{\alpha + y + \beta + n - y} =  \frac{\alpha + y}{\alpha  + \beta + n }= \frac{1+4}{1+1+14}=\frac{5}{16}= 0.3125
$$

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
summarize_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
#Guardar el resumen en un objeto
resumen1 <- summarize_beta_binomial(alpha = 1, beta = 1, y = 4, n = 14)

#Crear la tabla estilizada
kable(resumen1, 
      digits = 4, #Redondear los decimales a 4 para mayor claridad
      caption = "Medidas de resumen beta-binomial(1, 1, 4, 14)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), #Estilo
                full_width = FALSE) #Evita que la tabla ocupe toda la página
```

De esta manera comprobamos que la esperanza hallada de manera teórica es igual a la esperanza hallada por la función summarize. 

Se refuerza la información que vimos en el gráifco del prior y posterior. Si bien comentamos que la moda del posterior estaba entorno a 0.25, ahora visualizamos que es exactamente 0.2857. Por otro lado se reafirma la baja varianza que comentamos.

Finalemnte, es correcto que no exista el modo del prior, dado que es una beta(1,1) y esta perfectamente equidistribuida.

### Parte dos: Calculo para 1990

Busqueda de la informacion para 1990 (Verosimilitud)

```{r, echo= FALSE, warning=FALSE, message=FALSE}
bechdel %>% 
  filter(year == 1990) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

Partimos del posterior anterior que ya sabemos como distribuye gracias a la fórmula del posterior en el modelo beta binomial y cargamos la verosimilitud buscada en el paso anterior. Dicho "posterior" se convirtió en nuestro nuevo prior y se agrega una nueva verosimilitud.

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_beta_binomial(alpha = 5, beta = 11, y = 6, n = 15)
```

Se observa que el psoterior está en un punto medio entre el priori y la verosimiitud. Entendemos que esto es correcto dado que el n dado no es lo suficientemente grande para volcar los datos hacia la verosimilitud, y además el prior y la verosimilitud se encuentran ingualmente concentrados en torno a un valor, ninguno es mucho más disperso ni concentrado que el otro.


```{r, echo= FALSE, warning=FALSE, message=FALSE, include=FALSE}
summarize_beta_binomial(5, 11, 6, 15)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
resumen2 <- summarize_beta_binomial(alpha = 5, beta = 11, y = 6, n = 15)

kable(resumen2, 
      digits = 4, 
      caption = "Medidas de resumen beta-binomial(5, 11, 6, 15)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE) 
```

### Parte 3: Año 2000

Nuestro nuevo prior distribuirá Beta(11, 20)

```{r, echo= FALSE, warning=FALSE, message=FALSE}
bechdel %>% 
  filter(year == 2000) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_beta_binomial(alpha = 11, beta = 20 , y = 29, n = 63)

```

```{r, echo= FALSE, warning=FALSE, message=FALSE, include=FALSE}
summarize_beta_binomial(11, 20, 29, 63)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
resumen3 <- summarize_beta_binomial(alpha = 11, beta = 20, y = 29, n = 63)

kable(resumen3, 
      digits = 4, 
      caption = "Medidas de resumen beta-binomial(11, 20, 29, 63)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE) 
```

### Parte 4: Calculo de Jenna

```{r, echo= FALSE, warning=FALSE, message=FALSE}
bechdel %>% 
  filter(year %in% c(1980, 1990, 2000)) %>% 
  tabyl(binary) %>% 
  adorn_totals("row")
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_beta_binomial(alpha = 1, beta = 1 , y = 39, n = 92)

```

```{r, echo= FALSE, warning=FALSE, message=FALSE, include=FALSE}
summarize_beta_binomial(1, 1, 39, 92)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
resumen4 <- summarize_beta_binomial(alpha = 1, beta = 1, y = 39, n = 92)

kable(resumen4, 
      digits = 4, 
      caption = "Medidas de resumen beta-binomial(1, 1, 39, 92)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE) 
```

Se observa que Jenna llega a la misma posterior que John; probando que no importa el orden en el que se mire la información, se llegará a los mismos resultados. John realiza su análisis en tres días (pasos), mientras que Jenna sólo en uno. Pero al partir del mismo prior (Beta(1,1)) y utilizar las mismas muestras, llegan a los mismos posteriors.

Esta propiedad es denominada **propiedad de consistencia** o también **acumulación secuencial** y como comentamos anteriormente permite actualizar de una sola vez o en "capas". 

## Ejercicio 5.7: Womens world cup

Utilizaremos plot_gamma (1, 0.25) dado que es el prior dado.

```{r, echo= FALSE, warning=FALSE, message=FALSE}

plot_gamma(1, 0.25)

```

Se observa que $\lambda$ es decreciente y parte desde cero, con asimetría a la derecha. Es sumamente coherente con el contexto de que $\lambda$ representa la cantidad de goles promedio, sería extraño que 20 tenga probabilidad alta, por ejemplo. Sugiere que no habrá goles en la mayoría de partidos.

### Parte 2: Yi para modelo Poisson

Se utiliza el modelo Poisson para representar a Y dado que este es útil para los conteos. Y es una variable aleatoria para representar los goles.

Cada Yi es una observacio¿ón, es decir los goles de cada partido puntual. Por otro lado $\lambda$ representa la tasa media de ocurrencia, osea, la cantidad promedio de goles por partido.

### Parte 3: Total de goles por partido

```{r, echo= FALSE, warning=FALSE, message=FALSE}
library(fivethirtyeight)
data("wwc_2019_matches")
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
wwc_2019_matches <- wwc_2019_matches %>% 
  mutate(total_goals = score1 + score2)


wwc_2019_matches <- wwc_2019_matches %>% 
  tabyl(total_goals) %>% 
   adorn_totals("row")%>%
  filter (total_goals != "Total")

 
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
labels<-c("0","1","2","3","4","5","6","13")

ggplot(data=wwc_2019_matches, aes(x=total_goals, y= n)) + scale_x_discrete(label = labels)+ geom_point()+geom_col(width=0.01)
```

Se observan la cantidad de goles por partido y cuantas veces se repite ese resutado, es decir la frecuencia para cada resultado.

Para poder graficar el posterior y la verosimilitud necesitaremos calcular el total de goles que se convirtieron y el total de partidos. Este ultimo se observa que es 52 en wwc_2019_matches.

Para calcular el total de goles, debemos multriplcar la frecuencia de cada resultado por el total de goles de ese resultad yluego sumarlo.

```{r, echo= FALSE, warning=FALSE, message=FALSE}
wwc_2019_matches <- wwc_2019_matches %>%
  mutate (total_goals=as.numeric(total_goals)) %>%
  mutate (golesaux = n * total_goals)
sum_goles <- sum(wwc_2019_matches$golesaux, na.rm = TRUE)
```

"Sacamos" el valor NA que generamos antes al utilizar as.numeric (NO interesa graficarlo y alteraría la consistencia de nuestros datos)

```{r, echo= FALSE, warning=FALSE, message=FALSE}
wwc_2019_matches <- wwc_2019_matches %>%
  filter(!is.na(total_goals))
plot_poisson_likelihood(wwc_2019_matches$total_goals, 15)
```

Se observa que los valores más verosimiles de $\lambda$ rondan entre 3 y 5 goles de media por partido. Es una likelihood muy agresiva, en el sentido de que asigna probabilidad practicamente nula a 0 y 1 gol.

### Grafico del posterior

Ahora interesa calcular el posterior. Para ello utilizaremos la funcion plot_gamma_poisson.

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_gamma_poisson(1 , 0.25, sum_goles,52) + coord_cartesian(xlim = c(0, 10))
  
```

En este caso, el posterior y la likelihood son practicamente iguales. Recien al hacer zoom encontramos una pequena diferencia:

![Diferencia entre Posterior y likelihood](a.png){width="60%" height="40%" fig-align="center"}

Pensamos que esto ocurre por varios motivos: Tenemos un n considerablemente grande (52), la función de verosimilitud está sumamente concentrada entorno a un valor y el prior no está tan concentrado, sino que es más disperso.

De esta manera, hay un cambio radical en nuestro entendimiento de la media de goles por partido. De pasar de pensar que la media podia ser 0, 1 o 2, pasamos a poder afirmar que la media de goles por partido estara en el entorno de 3.

#Ejercicio 5.12: Control brains

Aclaración: Cuando la letra del ejercicio plantea "control subjects who have not been diagnosed with a concussion", entendemos que refiere al grupo denominado control, pero puede haber habido un error de interpretación.

Primero procederemos con cargar los datos y filtramos para el grupo de control.

```{r, echo= FALSE, warning=FALSE, message=FALSE}
data(football)
control_subjects <- football %>%
  filter(group == "control")
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
control_subjects %>%
  summarize(mean(volume))
```

El promedio de los sujetos que no tuvieron contusiones es de 7.6026 cm3 en una muestra de 25 personas.

```{r, echo= FALSE, warning=FALSE, message=FALSE}
ggplot(control_subjects, aes(x = volume)) + 
  geom_density()+coord_cartesian(xlim = c(3, 12))
```

Se observa que los valores van desde 6.175 hasta 9.71 y alcanzan el máximo de densidad entorno a 7.3, que es levemente menor al promedio (7.6026).

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_normal_likelihood(y = control_subjects$volume, sigma = 0.5)

```

De esta manera se visualiza la verosimilitud de u. Los datos reflejan que la media, que calculamos anteriormente es el valor mas probable.

### Calculo del posterior

Ya tenemos todo lo necesario para identificar el posterior: El prior tiene media θ=6.5 y τ=0.4 La muestra de los que no tuvieron contusion tiene media y=7.6026 y asumimos desvio conocido, pero por letra del ejercicio σ=0.5.

Sabemos que el Modelo Normal-Normal distribuye de la siguiente manera:

$$
\begin{aligned}
Y_i|\mu &\stackrel{ind}{\sim} N(\mu, \sigma^2) \\
\mu &\sim N(\theta, \tau^2)
\end{aligned}
$$

$$
\mu|\vec{y} \sim N\left(\theta\frac{\sigma^2}{n\tau^2 + \sigma^2} + \bar{y}\frac{n\tau^2}{n\tau^2 + \sigma^2}, \frac{\tau^2\sigma^2}{n\tau^2 + \sigma^2}\right)
$$

Por lo tanto, sustituyendo los valores que obtuvimos llegammos a que el modelo bayesiano conjugado Normal-Normal es el siguiente:

$$
\mu|\vec{y} \sim N\left(6.5\frac{0.5^2}{25*0.4^2 + 0.5^2} + \bar{7.6026}\frac{25*0.4^2}{25*0.4^2 + 0.5^2}, \frac{0.4^20.5^2}{25*0.4^2 + 0.5^2}\right)
$$ Entonces:

$$
\mu|\vec{y} \sim N(7.538, \ 0.0094)
$$

El grafico de nuestra distriubucion queda de la forma:

```{r, echo= FALSE, warning=FALSE, message=FALSE}
media_posterior <- 7.538
varianza_posterior <- 0.0094

#Calcular la desviación estándar (la función la necesita)
desviacion_estandar <- sqrt(varianza_posterior)

#Graficar la distribución
plot_normal(mean = media_posterior, sd = desviacion_estandar) +
  labs(
    title = "Distribución Posterior de μ",
    subtitle = "N(media = 7.538, varianza = 0.0094)",
    x = "Valor de μ",
    y = "Densidad"
  )
```

### Parte 3: Plot del prior, verosimilitud y posterior

```{r, echo= FALSE, warning=FALSE, message=FALSE}
plot_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,
                   y_bar = 7.6026, n = 25)
```

Nuevamente, dado que tenemos una verosimilitud cuyos datos están tan concentrados, el posterior se pega mucho a la verosimilitud, siendo estos muy similares en forma y media.

```{r, echo= FALSE, warning=FALSE, message=FALSE, include=FALSE}
summarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5,
                        y_bar = 7.6026, n = 25)
```

```{r, echo= FALSE, warning=FALSE, message=FALSE}
resunormal <- summarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, 
                                          y_bar = 7.6026, n = 25)

kable(resunormal, 
      digits = 4,
      caption = "Medidas de resumen para Normal-Normal(6.5, 0.4, 0.5, 7.6026, 25)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = FALSE) 

```

De esta manera confirmamos que nuestrio posterior calculado manualmente en la Parte 2 era correcto.

### Ejercicio Normal-Normal Conjugada

**Consigna:**

(i) Calcular la distribución a posteriori para μ.

(ii) Demostrar que la distribución a posteriori para μ es normal con media igual a un promedio ponderado de la media a priori θ y de la media muestral.

(iii) Analizar la influencia del tamaño muestral n sobre la media y la varianza de la distribución a posterior para μ. Conclusión: la distribución a priori normal para μ es conjugada con el modelo muestral de observaciones independientes de una población normal con media μ y varianza conocida σ2

**Respuestas:** Nos basamos en la guia del libro para demostrar la consigna paso a paso. Vamos a probar que la posteriori del modelo Normal-Normal sigue también una distribución normal

Primero, tenemos que la función de densidad de $\mu$ (posteriori) es proporcional al producto de la función de densidad priori de la Normal y la función de verosimilitud. Lo que queremos encontrar es la posteriori, por otro lado, el priori es lo que sabiamos de $\mu$ antes de ver los datos (la verosimilitud). Para todo $\mu$ perteneciente a los reales tenemos:

$$
\begin{aligned}
f(\mu|\vec{y}) &\propto f(\mu)L(\mu|\vec{y}) \propto \exp\left[-\frac{(\mu-\theta)^2}{2\tau^2}\right] \cdot \exp\left[-\frac{(\bar{y}-\mu)^2}{2\sigma^2/n}\right] \\[2ex]
\end{aligned}
$$

De forma resumida:

$$
\begin{aligned}
f(\mu|\vec{y}) \propto \underbrace{\exp\left[-\frac{(\mu-\theta)^2}{2\tau^2}\right]}_{\text{Nucleo del Prior}} \cdot \underbrace{\exp\left[-\frac{(\bar{y}-\mu)^2}{2\sigma^2/n}\right]}_{\text{Nucleo de la Verosimilitud}}
\end{aligned}
$$ 

El objetivo ahora es deshacernos de los paréntesis al cuadrado para poder agrupar los términos que contienen $\mu$. Para esto, se usa la fórmula del binomio conjugado: $(a-b)^2 = a^2 - 2ab + b^2$.

Al expandir los exponentes, obtenemos:

$-(\mu - \theta)^2 = -(\mu^2 - 2\mu\theta + \theta^2) = -\mu^2 + 2\mu\theta - \theta^2$

$-(\bar{y} - \mu)^2 = -(\mu - \bar{y})^2 = -(\mu^2 - 2\mu\bar{y} + \bar{y}^2) = -\mu^2 + 2\mu\bar{y} - \bar{y}^2$

$$
\begin{aligned}
f(\mu|\vec{y}) &\propto \exp\left[\frac{-\mu^2 + 2\mu\theta - \theta^2}{2\tau^2}\right] \exp\left[\frac{-\mu^2 + 2\mu\bar{y} - \bar{y}^2}{2\sigma^2/n}\right] \\
\end{aligned}
$$
Este siguiente paso es muy importante: "nos olvidaremos de las constantes". Como estamos trabajando con proporcionalidad con respecto a $\mu$ y los terminos $\theta^2$ y $\bar{y}^2$ no contienen ningun $\mu$, consideramos a estos como constantes. Esto va a simplificar mucho las cuentas. Obtenemos:

$$
\begin{aligned}
&\propto \exp\left[\frac{-\mu^2 + 2\mu\theta}{2\tau^2}\right] \exp\left[\frac{-\mu^2 + 2\mu\bar{y}}{2\sigma^2/n}\right] \\[2ex]
\end{aligned}
$$ 
A continuacion, hacemos denominador comun para luego poder aplicar las propiedades de potencia y sumar los exponentes: 

$$
\begin{aligned}
&\propto \exp\left[\frac{(-\mu^2 + 2\mu\theta)\sigma^2/n}{2\tau^2\sigma^2/n}\right] \exp\left[\frac{(-\mu^2 + 2\mu\bar{y})n\tau^2}{2\tau^2\sigma^2/n}\right] \\
&\propto \exp\left[\frac{(-\mu^2 + 2\mu\theta)\sigma^2 + (-\mu^2 + 2\mu\bar{y})n\tau^2}{2\tau^2\sigma^2}\right]
\end{aligned}
$$

Ahora, agrupamos los terminos mu y $\mu^2$ para simplificar:

$$
\begin{aligned}
f(\mu|\vec{y}) &\propto \exp\left[\frac{-\mu^2(n\tau^2 + \sigma^2) + 2\mu(\theta\sigma^2 + \bar{y}n\tau^2)}{2\tau^2\sigma^2}\right] \\
\end{aligned}
$$

¿Cómo podemos hacer para demostrar que lo que obtuvimos es una distribucion normal? Debemos lograr de alguna manera "hacer aparecer" el núcleo de la Normal. Un truco importante que se utiliza es el "complete the square" (completar el cuadrado). 

$$
\begin{aligned}
&\propto \exp\left[\frac{-\mu^2 + 2\mu\left(\frac{\theta\sigma^2 + \bar{y}n\tau^2}{n\tau^2 + \sigma^2}\right)}{2(\tau^2\sigma^2)/(n\tau^2 + \sigma^2)}\right] \\[3ex]
\end{aligned}
$$ 

Ahora podemos factorizar:

$$
\begin{aligned}
f(\mu|\vec{y}) &\propto \exp\left[-\frac{\left(\mu - \frac{\theta\sigma^2 + \bar{y}n\tau^2}{n\tau^2 + \sigma^2}\right)^2}{2(\tau^2\sigma^2)/(n\tau^2 + \sigma^2)}\right] \\[3ex]
\end{aligned}
$$

Obtenemos el núcleo de una función de densidad de una Normal para $\mu$. Utilizando esto podemos concluir que la distribución queda de esta forma:

$$
\begin{aligned}
\mu|\vec{y} &\sim N\left(\frac{\theta\sigma^2 + \bar{y}n\tau^2}{n\tau^2 + \sigma^2}, \frac{\tau^2\sigma^2}{n\tau^2 + \sigma^2}\right) \\[3ex]
\end{aligned}
$$ 

Por lo tanto, queda demostrado que la posteriori de una Normal-Normal conjugada es también una Normal.

Podemos reorganizar la media de la posteriori para demostrar que es el promedio ponderado de la media a priori de $\mu$ ($E(\mu) = \theta$) y la media de la muestra de $\bar{y}$:

**- El peso del prior depende de los datos**

**- El peso de los datos depende del prior y de la cantidad de datos**

$$
\begin{aligned}
\frac{\theta\sigma^2 + \bar{y}n\tau^2}{n\tau^2 + \sigma^2} &= \theta\frac{\sigma^2}{n\tau^2 + \sigma^2} + \bar{y}\frac{n\tau^2}{n\tau^2 + \sigma^2}
\end{aligned}
$$

Concluimos también que la varianza a posteriori obtiene información de la variabilidad a priori de $\tau$ y la variabilidad en los datos de $\sigma$. Ambos se ven afectados por el tamaño de la muestra, n.

En primer lugar, a medida que n aumente, la media a posteriori toma una ponderación menor en la media a priori y mayor ponderación en la media de la muestra $\bar{y}$. Tenemos entonces:

  $$
  \begin{aligned}
  \frac{\sigma^2}{n\tau^2 + \sigma^2} \to 0 \quad \text{y} \quad \frac{n\tau^2}{n\tau^2 + \sigma^2} \to 1
  \end{aligned}
  $$

Luego, cuando n aumenta, la varianza a posteriori disminuye:

$$
\begin{aligned}
\frac{\tau^2\sigma^2}{n\tau^2 + \sigma^2} \to 0
\end{aligned}
$$

Esto significa que, a medida que obtenemos mayor cantidad de datos, la certeza de la posterior con respecto a $\mu$ aumenta y se vuelve más cercana a los valores de la información.
