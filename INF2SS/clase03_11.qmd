---
title: "Regresion Simple Predictiva"
format: pdf
editor: visual
---

```{r}
# Load packages
library(bayesrules)
library(tidyverse)
library(rstan)
library(rstanarm)
library(bayesplot)
library(tidybayes)
library(janitor)
library(broom.mixed)
data(bikes)
```

```{r}
rm(list=ls())
# Cargar y graficar los datos
data(bikes)
ggplot(bikes, aes(x = temp_feel, y = rides)) + 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = TRUE)
```

```{r}
# Ajuste de un modelo bayesiano con stan_glm
bike_model <- stan_glm(rides ~ temp_feel, data = bikes, family = gaussian,
        prior_intercept = normal(5000, 1000),
        prior = normal(100, 40), 
        prior_aux = exponential(0.0008),
        chains = 4, iter = 5000*2, seed = 84735)
```

```{r}
# Tamaño efectivo de muestra y Rhat
neff_ratio(bike_model)  #bayesplot

rhat(bike_model)
```

```{r}
# Gráfico de cadenas paralelas (trazas)
mcmc_trace(bike_model, size = 0.1)
```

```{r}
# Gráfico de densidades superpuestas de las cadenas
mcmc_dens_overlay(bike_model)
```

```{r}
# Estadísticas resumen del posterior
tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```

```{r}
# Guardar las cuatro cadenas para cada parámetro en un data frame
bike_model_df <- as.data.frame(bike_model)

# Chequear el número de filas
nrow(bike_model_df)
```

```{r}
head(bike_model_df, 3)

```

```{r}
# 50 líneas simuladas del modelo
bikes %>%
  add_epred_draws(object = bike_model, ndraws = 150) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
  geom_line(aes(y = .epred, group = .draw), alpha = 0.15) + 
  geom_point(data = bikes, size = 0.05)
```

```{r}
# Tabular los valores de beta_1 que exceden 0  #janitor
bike_model_df %>% 
  mutate(exceeds_0 = temp_feel > 0) %>% 
  tabyl(exceeds_0)
# exceeds_0     n percent
#      TRUE 20000       1

```

```{r}
# Simular cuatro conjuntos de datos
bikes %>%
  add_predicted_draws(bike_model, ndraws = 4) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
  geom_point(aes(y = .prediction, group = .draw), size = 0.2) + 
  facet_wrap(~ .draw)
```

# Construccion del modelo predictivo a posteriori desde 0

Bike model df es el dataframe obtenido con los valores simulados via stan. cada fila representa un conjunto plausible de parámetros, que representan cada uno una regresión

```{r}
# Construir modelo predictivo a posteriori

first_set <- head(bike_model_df, 1)
first_set
```
Por ejemplo acá tomo el primer conjunto de parametros y hallo la  media esperada
```{r}
mu <- first_set$`(Intercept)` + first_set$temp_feel * 75
mu
```

Luego, para representar que no todos los das son de mu viajes, hago un muestreo aleatorio con la distribucion normal, obteniendo un nuevo "dato". 
```{r}
set.seed(84735)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
y_new
```

Teniendo en cuenta los valores de bikemodeldf, calculo 20.000 valores de mu y simulo 20.000 valores de ynew. 

```{r}
# Predecir viajes para cada conjunto de parámetros de la cadena
set.seed(84735)
predict_75 <- bike_model_df %>% 
  mutate(mu = `(Intercept)` + temp_feel*75,
         y_new = rnorm(20000, mean = mu, sd = sigma))

head(predict_75, 3)
```
Con eso construyo los intervalos de confianza de mu y de yneq 
```{r}
# Construir:
# intervalos de credibilidad para el número típico de viajes en bicicleta en un día de 75°F
# intervalos de predicción a posteriori para el número de viajes en bicicleta
predict_75 %>% 
  summarize(lower_mu = quantile(mu, 0.025),
            upper_mu = quantile(mu, 0.975),
            lower_new = quantile(y_new, 0.025),
            upper_new = quantile(y_new, 0.975))
#   lower_mu upper_mu lower_new upper_new
# 1 3842.508 4093.436  1492.104  6488.281
```

El primer gráfico (mu) muestra cómo varía la media esperada del número de viajes entre los distintos escenarios del modelo.
Suele ser una curva más estrecha y concentrada (porque la media es bastante estable)
```{r}
# Graficar el modelo posterior de viajes típicos para 75°F
ggplot(predict_75, aes(x = mu)) + 
  geom_density()
```
El segundo gráfico (y_new) muestra cómo varían las predicciones reales simuladas.
Esta curva es más ancha y dispersa, porque incluye la variabilidad de los datos reales además de la de los parámetros.
```{r}
# Graficar el modelo predictivo posterior de viajes para mañana
ggplot(predict_75, aes(x = y_new)) + 
  geom_density()
```

```{r}
library(gridExtra)
# Calcular la densidad de mu para obtener el límite Y máximo
dens_mu <- density(predict_75$mu)
ymax <- max(dens_mu$y)
```

```{r}
# Primer gráfico: densidad de mu
p1 <- ggplot(predict_75, aes(x = mu)) +
  geom_density() +
  ylim(0, ymax) +
  labs(title = "Posteriori de mu (media esperada)")

```

```{r}
# Segundo gráfico: densidad de y_new (predicción posterior)
p2 <- ggplot(predict_75, aes(x = y_new)) +
  geom_density() +
  ylim(0, ymax) +
  labs(title = "Posteriori predictiva (y_new)")

```


El gráfico combinado (grid.arrange(p1, p2)) te permite ver la diferencia entre:
la incertidumbre del modelo (mu), y la incertidumbre del fenómeno real (y_new).

En general: Posterior de mu: mide lo que el modelo “cree” sobre el promedio de viajes.
Posterior predictiva de ynew: mide lo que el modelo espera observar en la práctica un día cualquiera con 75 °F.


```{r}
# Comparación dos gráfico modelos a posteriori
grid.arrange(p1, p2, nrow = 1)
```       


Explorar las distribuciones a priori que usa rstanarm: Con prior_summary() podés revisar sus parámetros antes de ajustarlo.

```{r}
# Empleo de las distribuciones a priori ajustadas propuestas por rstanarm

bike_model_default <- stan_glm(
  rides ~ temp_feel, data = bikes, 
  family = gaussian,
  prior_intercept = normal(5000, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 84735)

prior_summary(bike_model_default)
```
Simula sin usar datos:
bike_default_priors “simulá resultados posibles solo con la información de las priors, sin usar datos observados”.
```{r}
# Simular a partir de los priors
bike_default_priors <- update(bike_model_default, prior_PD = TRUE)

```

Muestra cómo el modelo “imagina” que se vería la relación entre temperatura y viajes antes de ver los datos reales.
Debería aparecer un abanico de líneas muy disperso, reflejando una creencia muy vaga.

```{r}
# 200 líneas simuladas del modelo con las priori
bikes %>%
add_epred_draws(object = bike_default_priors, ndraws = 200) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
  geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
```

Después, simulás 4 datasets completos generados únicamente desde las priors, es coherente y logico que sean tan diferentes. 

```{r}
# 4 conjuntos de datos simulados usando únicamente las priori
set.seed(3)
bikes %>%
  add_predicted_draws(bike_default_priors, ndraws = 4) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
  geom_point(aes(y = .prediction, group = .draw)) + 
  facet_wrap(~ .draw)

```

Toma una muestra posterior concreta de parámetros, calcula la media esperada y genera valores simulados ysim de viajes (simulated_rides) bajo ese modelo.

Luego compara la distribución simulada con la distribución real de los datos (rides).

El gráfico tiene dos curvas:

Celeste (lightblue) → distribución de viajes simulados por el modelo.

Azul oscuro (darkblue) → distribución real de viajes observados.

Si las curvas se parecen, significa que el modelo representa bien la variabilidad real de los datos (buena calibración).
Si difieren mucho, el modelo probablemente viola algún supuesto (por ejemplo, error normal, varianza constante, etc.).


```{r}
## Comprobación de los supuestos del modelo
first_set <- head(bike_model_df, 1)
first_set

beta_0 <- first_set$`(Intercept)`
beta_1 <- first_set$temp_feel
sigma  <- first_set$sigma
set.seed(84735)
one_simulation <- bikes %>% 
  mutate(mu = beta_0 + beta_1 * temp_feel,
         simulated_rides = rnorm(500, mean = mu, sd = sigma)) %>% 
  select(temp_feel, rides, simulated_rides)

head(one_simulation, 2)

ggplot(one_simulation, aes(x = simulated_rides)) + 
  geom_density(color = "lightblue") + 
  geom_density(aes(x = rides), color = "darkblue")
```

Esta función automatiza lo anterior: genera 50 conjuntos simulados de datos (yrep) a partir del modelo posterior y los compara con los datos reales.
Es una validación visual global del modelo:
```{r}
# Se examinan 50 de las 20000 muestras simuladas
pp_check(bike_model, nreps = 50) + 
  xlab("rides")
```
Extrae un día concreto del dataset —por ejemplo, el 22 de octubre de 2012— para usarlo como caso de validación individual.

Podés usar esa observación puntual para:

Compararla con las predicciones posteriores del modelo.

Evaluar si el número real de viajes (6 228) se encuentra dentro del intervalo predictivo posterior generado para un día de 75 °F.
```{r}
## Resúmenes predictivos a posteriori
bikes %>% 
  filter(date == "2012-10-22") %>% 
  select(temp_feel, rides)
#   temp_feel rides
# 1     75.46  6228
```

Generás 20 000 simulaciones de viajes para un día de 75 °F, integrando la incertidumbre de los parámetros

```{r}
# Simular el modelo predictivo a posteriori
set.seed(84735)
predict_75 <- bike_model_df %>% 
  mutate(mu = `(Intercept)` + temp_feel*75,
         y_new = rnorm(20000, mean = mu, sd = sigma))
```

Buscás la densidad en el punto donde el valor observado fue 6228 viajes (22 oct 2012).

```{r}
# Calcular densidad explícitamente
dens <- density(predict_75$y_new)
# Encontrar la y correspondiente a x=6228
# Si 6228 no está exactamente en x, se interpola:
yline <- approx(x = dens$x, y = dens$y, xout = 6228)$y
```
Basicamente estoy viendo que tan probable era obtener ese valor que observe para un dia x, comparandolo contra la simulación de 20.000 casos. 
```{r}
# Graficar el modelo predictivo a posteriori
ggplot(predict_75, aes(x = y_new)) +
  geom_density() +
  annotate("segment", x = 6228, xend = 6228,
           y = 0, yend = yline,
           color = "#2171b5", size = 1.1) +
  labs(x = expression(y[new]), y = "Densidad")
```

```{r}
predict_75 %>% 
  summarize(mean = mean(y_new), error = 6228 - mean(y_new))
#  mean error
#1 3967  2261

predict_75 %>% 
  summarize(sd = sd(y_new), error = 6228 - mean(y_new), error_scaled = error / sd(y_new))
#     sd error error_scaled
# 1 1280  2261        1.767

predict_75 %>% 
  summarize(lower_95 = quantile(y_new, 0.025),
            lower_50 = quantile(y_new, 0.25),
            upper_50 = quantile(y_new, 0.75),
            upper_95 = quantile(y_new, 0.975))
```
Como 6228 el valor que elegimos cae dentro delos intervalos no es tan extremo.


## Verificacion predictiva por intervalos
Esto genera 20000 × 500 predicciones (todas las observaciones, todos los draws).
ppc_intervals() grafica bandas de intervalo:

Bandas más oscuras: intervalo del 50 % (zona más probable).

Bandas claras: intervalo del 95 %.

Puntos negros: datos reales.

Si los puntos caen dentro de las bandas, el modelo predice bien para ese rango de temperaturas.
Si muchos puntos quedan fuera, hay sesgo o mala calibración.

```{r}
set.seed(84735)
predictions <- posterior_predict(bike_model, newdata = bikes)
dim(predictions)
#[1] 20000   500

ppc_intervals(bikes$rides, yrep = predictions, x = bikes$temp_feel, prob = 0.5, prob_outer = 0.95)
```
### Validacion con una muestra de 75 días
Esto repite el análisis anterior, pero con un subconjunto aleatorio de 25 días para ver más claramente cómo se comparan las predicciones con los valores reales.
Es útil para visualizar la dispersión individual sin saturar el gráfico.
```{r}
set.seed(84735)
# Seleccionar muestra aleatoria de 25 días
bikes_25 <- bikes %>% sample_n(25)

# Predicciones posteriores sólo para esos 25 días
predictions_25 <- posterior_predict(bike_model, newdata = bikes_25)

# Opcional: chequear las dimensiones (debería ser 20000 x 25)
dim(predictions_25)
```

```{r}
# Gráfico PPC intervals para la muestra de 25 días
ppc_intervals(
  y = bikes_25$rides,
  yrep = predictions_25,
  x = bikes_25$temp_feel,
  prob = 0.5,
  prob_outer = 0.95
)
```

Metricas de resumen: 
Métrica	Significado
MAE	Error absoluto medio (cuántos viajes, en promedio, difiere el modelo de la realidad).
mae_scaled	MAE escalado (relativo al rango o desviación típica).
within_50	Proporción de observaciones dentro del intervalo del 50 %.
within_95	Proporción dentro del 95 %.
```{r}
# Resúmenes predictivos posteriores
set.seed(84735)
prediction_summary(bike_model, data = bikes)
#     mae mae_scaled within_50 within_95
# 1 989.7     0.7712     0.438     0.968
```


### Validacion cruzada:

Esto divide los datos en 10 grupos, entrena el modelo 10 veces y evalúa la capacidad predictiva fuera de muestra.

El MAE promedio entre pliegues es ≈ 1 029 viajes, muy similar al del conjunto completo → el modelo generaliza bien.

La cobertura del 95 % se mantiene ≈ 0.97 → buena calibración.
```{r}
set.seed(84735)
cv_procedure <- prediction_summary_cv(
  model = bike_model, data = bikes, k = 10)
cv_procedure$folds
#fold    mae mae_scaled within_50 within_95
#1     1  990.2     0.7699      0.46      0.98
#2     2  963.8     0.7423      0.40      1.00
#3     3  951.3     0.7300      0.42      0.98
#4     4 1018.6     0.7910      0.46      0.98
#5     5 1161.5     0.9091      0.36      0.96
#6     6  937.6     0.7327      0.46      0.94
#7     7 1270.9     1.0061      0.32      0.96
#8     8 1111.9     0.8605      0.36      1.00
#9     9 1098.7     0.8679      0.40      0.92
#10   10  786.8     0.6060      0.56      0.96
cv_procedure$cv
#     mae mae_scaled within_50 within_95
# 1 1029     0.8015      0.42     0.968
```

LOO (Leave-One-Out Cross-Validation)

Parámetro	Significado
elpd_loo	Expected log predictive density: medida global de ajuste predictivo (más alto es mejor).
p_loo	Complejidad efectiva del modelo (número efectivo de parámetros).
looic	Criterio de información LOO (más bajo es mejor).
→ Es un modelo simple (≈2.5 parámetros efectivos) con ajuste adecuado; no hay evidencia de sobreajuste.
```{r}
# Resúmenes predictivos posteriores para los datos originales
set.seed(84735)
prediction_summary(bike_model, data = bikes)
#     mae mae_scaled within_50 within_95
# 1 989.7     0.7712     0.438     0.968
model_elpd <- loo(bike_model)
model_elpd$estimates
#Estimación      EE
#elpd_loo  -4289.0 13.1186
#p_loo         2.5  0.1638
#looic      8578.1 26.2371
```
